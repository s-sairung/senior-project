{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are configurable:\n",
    "\n",
    "| Parameter | Description |\n",
    "| --------- | ----------- |\n",
    "| `train_samples_per_class` | Amount of samples per class of the training set |\n",
    "| `val_samples_per_class` | Amount of samples per class of the validation set |\n",
    "| `dir_in` | Dataset directory with huge amount of data |\n",
    "| `dir_out` | Directory for sampled data |\n",
    "\n",
    "Note: This script assumes conventional folder structure:\n",
    "```\n",
    "dataset (e.g. coco)\n",
    "├── train\n",
    "│   ├── labels\n",
    "│   └── images\n",
    "└── val\n",
    "    ├── labels\n",
    "    └── images\n",
    "```\n",
    "\n",
    "Note 2: Only the `train` folder is copied to the output. The `val` folder is untouched by this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_per_class = 500\n",
    "val_samples_per_class = 200\n",
    "dir_in = \"./dataset/coco\"\n",
    "dir_out = \"./dataset/coco_sampling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_folder_in = os.path.join(dir_in, \"train\")\n",
    "train_image_folder_in = os.path.join(train_folder_in, \"images\")\n",
    "train_label_folder_in = os.path.join(train_folder_in, \"labels\")\n",
    "train_label_files = os.listdir(train_label_folder_in)\n",
    "\n",
    "train_folder_out = os.path.join(dir_out, \"train\")\n",
    "train_image_folder_out = os.path.join(train_folder_out, \"images\")\n",
    "train_label_folder_out = os.path.join(train_folder_out, \"labels\")\n",
    "\n",
    "\n",
    "val_folder_in = os.path.join(dir_in, \"val\")\n",
    "val_image_folder_in = os.path.join(val_folder_in, \"images\")\n",
    "val_label_folder_in = os.path.join(val_folder_in, \"labels\")\n",
    "val_label_files = os.listdir(val_label_folder_in)\n",
    "\n",
    "val_folder_out = os.path.join(dir_out, \"val\")\n",
    "val_image_folder_out = os.path.join(val_folder_out, \"images\")\n",
    "val_label_folder_out = os.path.join(val_folder_out, \"labels\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Files Dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize dictionary variable**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will attempt to read variable from a file. If none is found, it will initialize a blank dictionary, meaning you have to manually generate a new one using the code block below. The dictionary variable acts like a cache, and is used in the random sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dict_found = False\n",
    "\n",
    "try:\n",
    "    f = open(\"sampling_dict.pckl\", \"rb\")\n",
    "    class_files = pickle.load(f)\n",
    "    f.close()\n",
    "    print(\"Dictionary variable has been loaded from disk.\")\n",
    "    dict_found = True\n",
    "except (FileNotFoundError, EOFError) as e:\n",
    "    class_files = {\"train\": {}, \"val\": {}}\n",
    "    print(\"Dictionary variable has not been loaded from disk. Manual generation is required.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate files dictionary and save dictionary variable into a file**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if the stored dictionary variable is not available for use, or when you made changes to the dataset files, or when you simply want to generate a fresh new one. This process might take a while depending on the number of files. When everything is done, the data file will be created on disk. As mentioned, this acts like a cache so that you don't have to re-read every label files when re-generate random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if dict_found:\n",
    "    print(\"Dictionary variable generation is not required.\")\n",
    "\n",
    "else:\n",
    "    print(\"Generating dictionary variable.\")\n",
    "    for filename in tqdm(train_label_files):\n",
    "        filepath = os.path.join(train_label_folder_in, filename)\n",
    "        label_file = open(filepath, \"r\")\n",
    "        labels = label_file.readlines()\n",
    "        for line in labels:\n",
    "            label_class = int(line.split(\" \")[0])\n",
    "            try:\n",
    "                class_files[\"train\"][label_class].add(filename)\n",
    "            except KeyError:\n",
    "                class_files[\"train\"][label_class] = {filename}\n",
    "        label_file.close()\n",
    "\n",
    "    for filename in tqdm(val_label_files):\n",
    "        filepath = os.path.join(val_label_folder_in, filename)\n",
    "        label_file = open(filepath, \"r\")\n",
    "        labels = label_file.readlines()\n",
    "        for line in labels:\n",
    "            label_class = int(line.split(\" \")[0])\n",
    "            try:\n",
    "                class_files[\"val\"][label_class].add(filename)\n",
    "            except KeyError:\n",
    "                class_files[\"val\"][label_class] = {filename}\n",
    "        label_file.close()\n",
    "\n",
    "    f = open(\"sampling_dict.pckl\", \"wb\")\n",
    "    pickle.dump(class_files, f)\n",
    "    f.close()\n",
    "\n",
    "    print(\"Dictionary variable has been saved to disk.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The number of output samples might be less than `samples_per_class * number_of_classes` as one label file could contain more than one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_sample_labels = set()\n",
    "val_sample_labels = set()\n",
    "\n",
    "for class_index in class_files[\"train\"]:\n",
    "    try:\n",
    "        samples = random.sample([*class_files[\"train\"][class_index]], train_samples_per_class)\n",
    "    except ValueError as e:\n",
    "        samples = [*class_files[\"train\"][class_index]]\n",
    "    train_sample_labels = train_sample_labels.union(samples)\n",
    "    \n",
    "print(f\"Successfully picked {len(train_sample_labels)} train random samples.\")\n",
    "\n",
    "for class_index in class_files[\"val\"]:\n",
    "    try:\n",
    "        samples = random.sample([*class_files[\"val\"][class_index]], val_samples_per_class)\n",
    "    except ValueError as e:\n",
    "        samples = [*class_files[\"val\"][class_index]]\n",
    "    val_sample_labels = val_sample_labels.union(samples)\n",
    "\n",
    "print(f\"Successfully picked {len(val_sample_labels)} val random samples.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Files to the Destination Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "dirs = [dir_out, train_label_folder_out, train_image_folder_out, val_image_folder_out, val_label_folder_out]\n",
    "\n",
    "for d in dirs:\n",
    "    if not os.path.isdir(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "for filename in tqdm(train_sample_labels):\n",
    "    label_source_path = os.path.join(train_label_folder_in, filename)\n",
    "    label_destination_path = os.path.join(train_label_folder_out, filename)\n",
    "    shutil.copyfile(label_source_path, label_destination_path)\n",
    "\n",
    "    filename_image = filename.removesuffix(\".txt\") + \".jpg\"\n",
    "    image_source_path = os.path.join(train_image_folder_in, filename_image)\n",
    "    image_destination_path = os.path.join(train_image_folder_out, filename_image)\n",
    "    shutil.copyfile(image_source_path, image_destination_path)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{timestamp}] Done copying train random samples from {train_folder_in} to {train_folder_out}.\")\n",
    "\n",
    "for filename in tqdm(val_sample_labels):\n",
    "    label_source_path = os.path.join(val_label_folder_in, filename)\n",
    "    label_destination_path = os.path.join(val_label_folder_out, filename)\n",
    "    shutil.copyfile(label_source_path, label_destination_path)\n",
    "\n",
    "    filename_image = filename.removesuffix(\".txt\") + \".jpg\"\n",
    "    image_source_path = os.path.join(val_image_folder_in, filename_image)\n",
    "    image_destination_path = os.path.join(val_image_folder_out, filename_image)\n",
    "    shutil.copyfile(image_source_path, image_destination_path)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"[{timestamp}] Done copying val random samples from {val_folder_in} to {val_folder_out}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916ee9c9cb7238f6d6b67c0fb5959fbcead487380aac231732270d002106c5da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
